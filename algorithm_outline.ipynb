{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "loving-discipline",
   "metadata": {},
   "source": [
    "## 1 — Recognize objects in image (or classify image)\n",
    "\n",
    "Using trained NN, get object label or labels for image, or otherwise provide a label for the image. Also store the centrality of the object. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-voltage",
   "metadata": {},
   "source": [
    "## 2  — Generate semantic word families\n",
    "\n",
    "For each label, use Word2Vec `similar` to retrieve list of words semantically related to the image object labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-chosen",
   "metadata": {},
   "source": [
    "## 3 — Generate all related words\n",
    "\n",
    "For each semantically related (below a distance threshold) word to each object label, measure its phonetic similarity to all words in the dictionary. Also store each words's distance.\n",
    "\n",
    "For each word in each semantic family, sort and choose the phonetically closest (below a distance threshold) words.\n",
    "(One way is to convert the word to IPA and compare to an IPA converted version of every word in the CMU dictionary.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-mississippi",
   "metadata": {},
   "source": [
    "## 4 — Gather candidate phrases\n",
    "\n",
    "For each word in the phonetic family, of each word in the semantic family, of each object label, retrieve each idiom containing the word.\n",
    "Add the idiom Id, as well as the stats on the object centrality, semantic distance, and phonetic distance, to a dataframe.\n",
    "\n",
    "Compute _suitability score_ for each word-idiom match and add this to that column of the dataframe\n",
    "\n",
    "Also, for each word in the semantic family, search the joke list for match and add that these to a joke_match dataframe, to use if there's too low a suitability score using a substitution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-application",
   "metadata": {},
   "source": [
    "## 5 — Choose captions\n",
    "\n",
    "Sort captions dataframe by the _suitability score_\n",
    "\n",
    "Choose the top 10 and generate a list containing each caption with the original semantic family word substituted into the idiom in addition to jokes containing any of the semantic family words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lovely-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_topic=  'two'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polished-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-waste",
   "metadata": {},
   "source": [
    "## -1  — Webscrape and process phrases (idioms, sayings, aphorisms)\n",
    "\n",
    "They should be converted into lists of phonetic sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-omega",
   "metadata": {},
   "source": [
    "## 0  — Load `phrase_dict` pickled and processed after being scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-indication",
   "metadata": {},
   "source": [
    "#### Data structures defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polish-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Phrase = namedtuple('Phrase',['text_string', 'word_list','phon_list','string_length', 'word_count', 'prefix', 'phrase_type'])\n",
    "phrase_dict = dict()\n",
    "\n",
    "Close_word = namedtuple('Close_word', ['word', 'distance'])\n",
    "\n",
    "Sem_family = namedtuple('Sem_family', ['locus_word', 'sem_fam_words'])\n",
    "\n",
    "Phon_family = namedtuple('Phon_family', ['locus_word', 'close_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-browser",
   "metadata": {},
   "source": [
    "#### Temporary toy example of the dict of phrases, to be replaced with idioms etc. scraped from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eastern-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_string = 'Smarter than the average bear'\n",
    "w_list = t_string.lower().split()\n",
    "ph_id1 = uuid.uuid1()\n",
    "phrase_dict[ph_id1] = Phrase(text_string = t_string, word_list = w_list, phon_list = w_list, string_length = len(t_string), word_count = len(w_list), prefix=\"As usual: \", phrase_type='idiom' )\n",
    "\n",
    "# toy example of the dict\n",
    "t_string = 'Not a hair out of place'\n",
    "w_list = t_string.lower().split()\n",
    "ph_id2 = uuid.uuid1()\n",
    "phrase_dict[ph_id2] = Phrase(text_string = t_string, word_list = w_list, phon_list = w_list, string_length = len(t_string), word_count = len(w_list), prefix=\"As usual: \", phrase_type='idiom' )\n",
    "\n",
    "# toy example of the dict\n",
    "t_string = 'Three blind mice'\n",
    "w_list = t_string.lower().split()\n",
    "ph_id3 = uuid.uuid1()\n",
    "phrase_dict[ph_id3] = Phrase(text_string = t_string, word_list = w_list, phon_list = w_list, string_length = len(t_string), word_count = len(w_list), prefix=\"As usual: \", phrase_type='idiom' )\n",
    "\n",
    "# toy example of the dict\n",
    "t_string = 'I just called to say I love you'\n",
    "w_list = t_string.lower().split()\n",
    "ph_id3 = uuid.uuid1()\n",
    "phrase_dict[ph_id3] = Phrase(text_string = t_string, word_list = w_list, phon_list = w_list, string_length = len(t_string), word_count = len(w_list), prefix=\"As usual: \", phrase_type='idiom' )\n",
    "\n",
    "t_string = 'Up, up in the air'\n",
    "w_list = t_string.lower().split()\n",
    "ph_id1 = uuid.uuid1()\n",
    "phrase_dict[ph_id1] = Phrase(text_string = t_string, word_list = w_list, phon_list = w_list, string_length = len(t_string), word_count = len(w_list), prefix=\"As usual: \", phrase_type='idiom' )\n",
    "\n",
    "t_string = 'Wouldn\\'t it be nice'\n",
    "w_list = t_string.lower().split()\n",
    "ph_id1 = uuid.uuid1()\n",
    "phrase_dict[ph_id1] = Phrase(text_string = t_string, word_list = w_list, phon_list = w_list, string_length = len(t_string), word_count = len(w_list), prefix=\"As usual: \", phrase_type='idiom' )\n",
    "\n",
    "t_string = 'Roses are red, violets are blue'\n",
    "w_list = t_string.lower().split()\n",
    "ph_id1 = uuid.uuid1()\n",
    "phrase_dict[ph_id1] = Phrase(text_string = t_string, word_list = w_list, phon_list = w_list, string_length = len(t_string), word_count = len(w_list), prefix=\"As usual: \", phrase_type='idiom' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-limitation",
   "metadata": {},
   "source": [
    "## 1 — Recognize objects in image (or classify image)\n",
    "\n",
    "Using trained NN, get object label or labels for image, or otherwise provide a label for the image. Also store the centrality of the object. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-thought",
   "metadata": {},
   "source": [
    "## 2 — Generate semantic word families\n",
    "\n",
    "For each label, use Word2Vec `similar` to retrieve list of words semantically related to the image object labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "awful-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonyms( w ):\n",
    "    return [l.name() for l in wordnet.synsets( w )[0].lemmas()]  # There may be other synonyms in the synset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-fraction",
   "metadata": {},
   "source": [
    "## 3 — Generate all related words\n",
    "\n",
    "For each semantically related (below a distance threshold) word to each object label, measure its phonetic similarity to all words in the dictionary. Also store each words's distance.\n",
    "\n",
    "For each word in each semantic family, sort and choose the phonetically closest (below a distance threshold) words.\n",
    "(One way is to convert the word to IPA and compare to an IPA converted version of every word in the CMU dictionary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "differential-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dictionary = ['two', 'pair', 'bear', 'scare', 'you', 'twice', 'hair', 'mice', 'speaker', 'book']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "armed-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_phon_fam = Phon_family(locus_word=Close_word('two', 3), close_words = [Close_word('you', 2.1)])\n",
    "# two_phon_fam\n",
    "\n",
    "# pair_phon_fam = Phon_family(locus_word=Close_word('pair', 5), close_words = [Close_word('bear', 1.5), Close_word('hair', 2.7)])\n",
    "# pair_phon_fam\n",
    "\n",
    "# twice_phon_fam = Phon_family(locus_word=Close_word('twice', 4.1), close_words = [Close_word('mice', 2.1)])\n",
    "# twice_phon_fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "royal-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "words_set = set( words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "traditional-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eng_to_ipa as ipa\n",
    "\n",
    "def syllable_count_diff( w1, w2 ):\n",
    "    return abs( ipa.syllable_count( w1 ) - ipa.syllable_count( w2 ))\n",
    "\n",
    "def same_syllable_count( w1, w2 ):\n",
    "    return syllable_count_diff(w1, w2) == 0\n",
    "\n",
    "def close_syllable_count( w1, w2, threshold=1):\n",
    "    return syllable_count_diff( w1, w2 ) <= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "angry-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventually will need to to filter for the word-frequency sweet-spot or at least for only Engllish words\n",
    "# Possibly rewrite with a decororater so that it uses memoization to speed this up\n",
    "\n",
    "# Rewrite this so it vectorizes the subtraction of the syllable counts\n",
    "\n",
    "def get_sized_rhymes( w ):\n",
    "    word_length_min = 2\n",
    "    rhyme_list = ipa.get_rhymes( w )\n",
    "    return [ [rhyme for rhyme  in rhyme_list if same_syllable_count( w, rhyme) and len(rhyme) >= word_length_min and rhyme in words_set]]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-romance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abandoned-infrared",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipa.isin_cmu('xue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "controversial-geneva",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['blue',\n",
       "  'boo',\n",
       "  'brew',\n",
       "  'chew',\n",
       "  'clue',\n",
       "  'coo',\n",
       "  'coup',\n",
       "  'crew',\n",
       "  'cue',\n",
       "  'dew',\n",
       "  'do',\n",
       "  'drew',\n",
       "  'due',\n",
       "  'ewe',\n",
       "  'few',\n",
       "  'flew',\n",
       "  'flu',\n",
       "  'flue',\n",
       "  'foo',\n",
       "  'fu',\n",
       "  'glue',\n",
       "  'gnu',\n",
       "  'goo',\n",
       "  'grew',\n",
       "  'gue',\n",
       "  'hew',\n",
       "  'hue',\n",
       "  'knew',\n",
       "  'leu',\n",
       "  'lew',\n",
       "  'lieu',\n",
       "  'loo',\n",
       "  'lue',\n",
       "  'mew',\n",
       "  'moo',\n",
       "  'mu',\n",
       "  'new',\n",
       "  'nu',\n",
       "  'pew',\n",
       "  'phew',\n",
       "  'phu',\n",
       "  'plew',\n",
       "  'pooh',\n",
       "  'pu',\n",
       "  'pugh',\n",
       "  'queue',\n",
       "  'rue',\n",
       "  'schuh',\n",
       "  'screw',\n",
       "  'shoe',\n",
       "  'shoo',\n",
       "  'skew',\n",
       "  'slew',\n",
       "  'spew',\n",
       "  'stew',\n",
       "  'strew',\n",
       "  'sue',\n",
       "  'thew',\n",
       "  'threw',\n",
       "  'through',\n",
       "  'true',\n",
       "  'view',\n",
       "  'whew',\n",
       "  'who',\n",
       "  'whoo',\n",
       "  'woo',\n",
       "  'yew',\n",
       "  'you',\n",
       "  'zoo']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_sized_rhymes('two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-aviation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fluid-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temporary stand-in function, to be replaced with one that computes phonetic distance\n",
    "\n",
    "# https://stackabuse.com/phonetic-similarity-of-words-a-vectorized-approach-in-python/\n",
    "# \"Phonetic Similarity of Words: A Vectorized Approach in Python\"\n",
    "\n",
    "def lev_dist( phon1, phon2 ):\n",
    "    return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "seven-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "\n",
    "# Are these three lines necessary?\n",
    "two_fam_member_list = ['you']\n",
    "pair_fam_member_list = ['bear', 'hair']\n",
    "twice_fam_member_list = ['mice']\n",
    "\n",
    "def make_phon_fam_for_sem_fam_member( w_record, thresh=.2 ):\n",
    "    w_phon_code = w_record.word # To be replaced with phonetic version if needed\n",
    "    close_word_list = []\n",
    "    \n",
    "    # Find words that are not necessarily rhyms but phonetically similar\n",
    "    for word in english_dictionary:\n",
    "        word_phon_code = word\n",
    "        dist = lev_dist(w_phon_code, word_phon_code)\n",
    "        if dist < thresh:\n",
    "            close_word_list.append( Close_word(word, dist) )\n",
    "    \n",
    "    rhyme_dist = .1 + random()/10\n",
    "    rhyme_word_list = get_sized_rhymes( w_record.word )[0]\n",
    "    \n",
    "    # Find words that are rhymes\n",
    "    for word in rhyme_word_list:\n",
    "         close_word_list.append( Close_word(word, rhyme_dist) )\n",
    "    \n",
    "    \n",
    "    return Phon_family(locus_word = w_record, close_words=close_word_list )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-cartridge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "manual-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_phon_fam = get_phon_fam_for_sem_fam_member( 'two' )\n",
    "# pair_phon_fam = get_phon_fam_for_sem_fam_member( 'pair' )\n",
    "# twice_phon_fam = get_phon_fam_for_sem_fam_member( 'twice' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "anticipated-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALERT:  Need to incorporate the semantic distance somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "blank-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_sem_fam = Sem_family(locus_word='two', phon_fams = [make_phon_fam_for_sem_fam_member( 'two' ), \\\n",
    "#                                                         make_phon_fam_for_sem_fam_member( 'pair' ), \\\n",
    "#                                                         make_phon_fam_for_sem_fam_member( 'twice' )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "welcome-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be replaced with Word2Vec `most_similar()`\n",
    "\n",
    "def get_most_similar_obsolete( w ):  \n",
    "    list_of_duples =  [('pair', .95), ('twice', .90)]\n",
    "    list_of_close_words = [Close_word( word=w_str, distance= 1 - w_sim) for w_str, w_sim in list_of_duples ]\n",
    "        \n",
    "    return list_of_close_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "appointed-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar( w ):  \n",
    "    list_of_duples = [(syn, 1) for syn in get_synonyms( w )]\n",
    "    if(w == 'two'):\n",
    "        additional_words =  [('pair', .95), ('twice', .90)]\n",
    "        list_of_duples.extend( additional_words )\n",
    "    list_of_close_words = [Close_word( word=w_str, distance= 1 - w_sim) for w_str, w_sim in list_of_duples ]\n",
    "        \n",
    "    return list_of_close_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "restricted-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_most_similar( 'three' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "decent-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_phon_fams_and_sem_family( w ):\n",
    "    word_record_ = Close_word(w, 0.0)\n",
    "    \n",
    "    sem_sim_words = get_most_similar( w )  # Eventually replace with call to Word2Vec\n",
    "    \n",
    "    #phon_fams_list = [make_phon_fam_for_sem_fam_member( word_record_  )]\n",
    "    phon_fams_list = []\n",
    "\n",
    "    \n",
    "    for close_w_record in sem_sim_words:\n",
    "        print( close_w_record )\n",
    "        phon_fams_list.append( make_phon_fam_for_sem_fam_member( close_w_record ) )\n",
    "    \n",
    "    return Sem_family(locus_word= word_record_, sem_fam_words = phon_fams_list)\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "certified-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close_word(word='two', distance=0)\n",
      "Close_word(word='2', distance=0)\n",
      "Close_word(word='II', distance=0)\n",
      "Close_word(word='deuce', distance=0)\n",
      "Close_word(word='pair', distance=0.050000000000000044)\n",
      "Close_word(word='twice', distance=0.09999999999999998)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sem_family(locus_word=Close_word(word='two', distance=0.0), sem_fam_words=[Phon_family(locus_word=Close_word(word='two', distance=0), close_words=[Close_word(word='blue', distance=0.1520560692386022), Close_word(word='boo', distance=0.1520560692386022), Close_word(word='brew', distance=0.1520560692386022), Close_word(word='chew', distance=0.1520560692386022), Close_word(word='clue', distance=0.1520560692386022), Close_word(word='coo', distance=0.1520560692386022), Close_word(word='coup', distance=0.1520560692386022), Close_word(word='crew', distance=0.1520560692386022), Close_word(word='cue', distance=0.1520560692386022), Close_word(word='dew', distance=0.1520560692386022), Close_word(word='do', distance=0.1520560692386022), Close_word(word='drew', distance=0.1520560692386022), Close_word(word='due', distance=0.1520560692386022), Close_word(word='ewe', distance=0.1520560692386022), Close_word(word='few', distance=0.1520560692386022), Close_word(word='flew', distance=0.1520560692386022), Close_word(word='flu', distance=0.1520560692386022), Close_word(word='flue', distance=0.1520560692386022), Close_word(word='foo', distance=0.1520560692386022), Close_word(word='fu', distance=0.1520560692386022), Close_word(word='glue', distance=0.1520560692386022), Close_word(word='gnu', distance=0.1520560692386022), Close_word(word='goo', distance=0.1520560692386022), Close_word(word='grew', distance=0.1520560692386022), Close_word(word='gue', distance=0.1520560692386022), Close_word(word='hew', distance=0.1520560692386022), Close_word(word='hue', distance=0.1520560692386022), Close_word(word='knew', distance=0.1520560692386022), Close_word(word='leu', distance=0.1520560692386022), Close_word(word='lew', distance=0.1520560692386022), Close_word(word='lieu', distance=0.1520560692386022), Close_word(word='loo', distance=0.1520560692386022), Close_word(word='lue', distance=0.1520560692386022), Close_word(word='mew', distance=0.1520560692386022), Close_word(word='moo', distance=0.1520560692386022), Close_word(word='mu', distance=0.1520560692386022), Close_word(word='new', distance=0.1520560692386022), Close_word(word='nu', distance=0.1520560692386022), Close_word(word='pew', distance=0.1520560692386022), Close_word(word='phew', distance=0.1520560692386022), Close_word(word='phu', distance=0.1520560692386022), Close_word(word='plew', distance=0.1520560692386022), Close_word(word='pooh', distance=0.1520560692386022), Close_word(word='pu', distance=0.1520560692386022), Close_word(word='pugh', distance=0.1520560692386022), Close_word(word='queue', distance=0.1520560692386022), Close_word(word='rue', distance=0.1520560692386022), Close_word(word='schuh', distance=0.1520560692386022), Close_word(word='screw', distance=0.1520560692386022), Close_word(word='shoe', distance=0.1520560692386022), Close_word(word='shoo', distance=0.1520560692386022), Close_word(word='skew', distance=0.1520560692386022), Close_word(word='slew', distance=0.1520560692386022), Close_word(word='spew', distance=0.1520560692386022), Close_word(word='stew', distance=0.1520560692386022), Close_word(word='strew', distance=0.1520560692386022), Close_word(word='sue', distance=0.1520560692386022), Close_word(word='thew', distance=0.1520560692386022), Close_word(word='threw', distance=0.1520560692386022), Close_word(word='through', distance=0.1520560692386022), Close_word(word='true', distance=0.1520560692386022), Close_word(word='view', distance=0.1520560692386022), Close_word(word='whew', distance=0.1520560692386022), Close_word(word='who', distance=0.1520560692386022), Close_word(word='whoo', distance=0.1520560692386022), Close_word(word='woo', distance=0.1520560692386022), Close_word(word='yew', distance=0.1520560692386022), Close_word(word='you', distance=0.1520560692386022), Close_word(word='zoo', distance=0.1520560692386022)]), Phon_family(locus_word=Close_word(word='2', distance=0), close_words=[]), Phon_family(locus_word=Close_word(word='II', distance=0), close_words=[]), Phon_family(locus_word=Close_word(word='deuce', distance=0), close_words=[Close_word(word='boose', distance=0.13055929944741368), Close_word(word='cruce', distance=0.13055929944741368), Close_word(word='goose', distance=0.13055929944741368), Close_word(word='hoose', distance=0.13055929944741368), Close_word(word='juice', distance=0.13055929944741368), Close_word(word='loose', distance=0.13055929944741368), Close_word(word='luce', distance=0.13055929944741368), Close_word(word='moose', distance=0.13055929944741368), Close_word(word='mousse', distance=0.13055929944741368), Close_word(word='noose', distance=0.13055929944741368), Close_word(word='nous', distance=0.13055929944741368), Close_word(word='sluice', distance=0.13055929944741368), Close_word(word='spruce', distance=0.13055929944741368), Close_word(word='truce', distance=0.13055929944741368), Close_word(word='use', distance=0.13055929944741368)]), Phon_family(locus_word=Close_word(word='pair', distance=0.050000000000000044), close_words=[Close_word(word='aer', distance=0.1267643253108314), Close_word(word='air', distance=0.1267643253108314), Close_word(word='bare', distance=0.1267643253108314), Close_word(word='bear', distance=0.1267643253108314), Close_word(word='blair', distance=0.1267643253108314), Close_word(word='blare', distance=0.1267643253108314), Close_word(word='care', distance=0.1267643253108314), Close_word(word='chair', distance=0.1267643253108314), Close_word(word='dare', distance=0.1267643253108314), Close_word(word='darr', distance=0.1267643253108314), Close_word(word='ere', distance=0.1267643253108314), Close_word(word='err', distance=0.1267643253108314), Close_word(word='eyre', distance=0.1267643253108314), Close_word(word='fair', distance=0.1267643253108314), Close_word(word='fare', distance=0.1267643253108314), Close_word(word='flair', distance=0.1267643253108314), Close_word(word='flare', distance=0.1267643253108314), Close_word(word='gair', distance=0.1267643253108314), Close_word(word='gare', distance=0.1267643253108314), Close_word(word='glare', distance=0.1267643253108314), Close_word(word='hair', distance=0.1267643253108314), Close_word(word='haire', distance=0.1267643253108314), Close_word(word='hare', distance=0.1267643253108314), Close_word(word='heir', distance=0.1267643253108314), Close_word(word='lair', distance=0.1267643253108314), Close_word(word='lehr', distance=0.1267643253108314), Close_word(word='maire', distance=0.1267643253108314), Close_word(word='mare', distance=0.1267643253108314), Close_word(word='prayer', distance=0.1267643253108314), Close_word(word='rare', distance=0.1267643253108314), Close_word(word='sare', distance=0.1267643253108314), Close_word(word='scare', distance=0.1267643253108314), Close_word(word='share', distance=0.1267643253108314), Close_word(word='snare', distance=0.1267643253108314), Close_word(word='spare', distance=0.1267643253108314), Close_word(word='square', distance=0.1267643253108314), Close_word(word='stair', distance=0.1267643253108314), Close_word(word='stare', distance=0.1267643253108314), Close_word(word='swear', distance=0.1267643253108314), Close_word(word='tear', distance=0.1267643253108314), Close_word(word='their', distance=0.1267643253108314), Close_word(word='there', distance=0.1267643253108314), Close_word(word='ware', distance=0.1267643253108314), Close_word(word='wear', distance=0.1267643253108314), Close_word(word='where', distance=0.1267643253108314)]), Phon_family(locus_word=Close_word(word='twice', distance=0.09999999999999998), close_words=[Close_word(word='bice', distance=0.1332124921387603), Close_word(word='dice', distance=0.1332124921387603), Close_word(word='dyce', distance=0.1332124921387603), Close_word(word='feis', distance=0.1332124921387603), Close_word(word='gneiss', distance=0.1332124921387603), Close_word(word='grice', distance=0.1332124921387603), Close_word(word='ice', distance=0.1332124921387603), Close_word(word='mice', distance=0.1332124921387603), Close_word(word='nice', distance=0.1332124921387603), Close_word(word='price', distance=0.1332124921387603), Close_word(word='rice', distance=0.1332124921387603), Close_word(word='slice', distance=0.1332124921387603), Close_word(word='spice', distance=0.1332124921387603), Close_word(word='splice', distance=0.1332124921387603), Close_word(word='thrice', distance=0.1332124921387603), Close_word(word='tice', distance=0.1332124921387603), Close_word(word='trice', distance=0.1332124921387603), Close_word(word='vice', distance=0.1332124921387603), Close_word(word='vise', distance=0.1332124921387603), Close_word(word='wice', distance=0.1332124921387603)])])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sem_fam = make_phon_fams_and_sem_family('two')\n",
    "two_sem_fam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-medline",
   "metadata": {},
   "source": [
    "## 4 — Gather candidate phrases\n",
    "\n",
    "For each word in the phonetic family, of each word in the semantic family, of each object label, retrieve phrases containing the word.\n",
    "Add the phrase_Id, as well as the stats on the object centrality, semantic distance, and phonetic distance, to a dataframe.\n",
    "\n",
    "Compute _suitability score_ for each word-phrase match and add this to that column of the dataframe\n",
    "\n",
    "Also, for each word in the semantic family, search the joke list for match and add that these to a joke_match dataframe, to use if there's too low a suitability score using a substitution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-produce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "assumed-favorite",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "prime-yemen",
   "metadata": {},
   "source": [
    "## TO CODE NEXT\n",
    "\n",
    "Write code that takes the word `twice` and returns its `semantic_family` which is a list of words \n",
    "('pair', and 'twice' in this case) and returns either (TBD) the list phonetically similar words or \n",
    "the pboneticized version of the word to be compared with the phoneticized variants of words in\n",
    "the phrases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-ambassador",
   "metadata": {},
   "source": [
    "#### Define dataframe for candidate phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "south-payday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>semantic_match</th>\n",
       "      <th>sem_dist</th>\n",
       "      <th>phonetic_match</th>\n",
       "      <th>phon_dist</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [semantic_match, sem_dist, phonetic_match, phon_dist, phrase_id, score]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['semantic_match', 'sem_dist', 'phonetic_match', 'phon_dist', 'phrase_id', 'score']\n",
    "\n",
    "cand_df = pd.DataFrame(columns= col_names)\n",
    "cand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-thumb",
   "metadata": {},
   "source": [
    "#### Need to write body of function that will convert to phoneticized version of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "valid-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phoneticized( w ):\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-dialogue",
   "metadata": {},
   "source": [
    "### ALERT:  Instead, pre-generate a dictionary of phoneticized versions of the words in the list of idioms. Each phonetic word should map to a list of duples each of which is a phrase id and the corresponding word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "agreed-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_phrases( w ):\n",
    "    matched_id_list = []\n",
    "    for phrase_id in phrase_dict.keys():\n",
    "        if w in phrase_dict[phrase_id].phon_list:\n",
    "            matched_id_list.append(phrase_id)\n",
    "    return matched_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bulgarian-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_matching_phrases('bear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "stopped-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cycles through each phonetic family in the semantic family to get matching phrases\n",
    "\n",
    "#def get_phrases_for_phon_fam( phon_fam_, sem_dist_ ):\n",
    "def get_phrases_for_phon_fam( phon_fam_ ):\n",
    "\n",
    "    word_match_records_ = []\n",
    "\n",
    "    #phon_fam_ = pair_phon_fam\n",
    "    for word in phon_fam_.close_words:\n",
    "        matched_phrases = get_matching_phrases( word.word )\n",
    "        #print(word, len(matched_phrases))\n",
    "        if matched_phrases:\n",
    "            for p_id in matched_phrases:\n",
    "                word_match_records_.append({'semantic_match': phon_fam_.locus_word.word, 'sem_dist': phon_fam_.locus_word.distance, 'phonetic_match': word.word, 'phon_dist': word.distance, 'phrase_id': p_id, 'score': ''})\n",
    "    return word_match_records_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "controlling-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases_for_sem_fam( sem_fam_ ):\n",
    "    word_match_records_ = []\n",
    "    for phon_fam_ in sem_fam_.sem_fam_words:\n",
    "        print( phon_fam_.locus_word.distance )\n",
    "        #word_match_records_.extend( get_phrases_for_phon_fam( phon_fam_, sem_fam_.locus_word.distance ) )\n",
    "        word_match_records_.extend( get_phrases_for_phon_fam( phon_fam_ ) )\n",
    "    return word_match_records_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "arctic-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_match_records = []   \n",
    "\n",
    "# word_match_records.extend( get_phrases_for_phon_fam( two_phon_fam ) ) \n",
    "# word_match_records.extend( get_phrases_for_phon_fam( pair_phon_fam ) )\n",
    "# word_match_records.extend( get_phrases_for_phon_fam( twice_phon_fam ) )  \n",
    "# word_match_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "recent-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cand_df = cand_df.append(word_match_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "short-profit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sem_family(locus_word=Close_word(word='two', distance=0.0), sem_fam_words=[Phon_family(locus_word=Close_word(word='two', distance=0), close_words=[Close_word(word='blue', distance=0.1520560692386022), Close_word(word='boo', distance=0.1520560692386022), Close_word(word='brew', distance=0.1520560692386022), Close_word(word='chew', distance=0.1520560692386022), Close_word(word='clue', distance=0.1520560692386022), Close_word(word='coo', distance=0.1520560692386022), Close_word(word='coup', distance=0.1520560692386022), Close_word(word='crew', distance=0.1520560692386022), Close_word(word='cue', distance=0.1520560692386022), Close_word(word='dew', distance=0.1520560692386022), Close_word(word='do', distance=0.1520560692386022), Close_word(word='drew', distance=0.1520560692386022), Close_word(word='due', distance=0.1520560692386022), Close_word(word='ewe', distance=0.1520560692386022), Close_word(word='few', distance=0.1520560692386022), Close_word(word='flew', distance=0.1520560692386022), Close_word(word='flu', distance=0.1520560692386022), Close_word(word='flue', distance=0.1520560692386022), Close_word(word='foo', distance=0.1520560692386022), Close_word(word='fu', distance=0.1520560692386022), Close_word(word='glue', distance=0.1520560692386022), Close_word(word='gnu', distance=0.1520560692386022), Close_word(word='goo', distance=0.1520560692386022), Close_word(word='grew', distance=0.1520560692386022), Close_word(word='gue', distance=0.1520560692386022), Close_word(word='hew', distance=0.1520560692386022), Close_word(word='hue', distance=0.1520560692386022), Close_word(word='knew', distance=0.1520560692386022), Close_word(word='leu', distance=0.1520560692386022), Close_word(word='lew', distance=0.1520560692386022), Close_word(word='lieu', distance=0.1520560692386022), Close_word(word='loo', distance=0.1520560692386022), Close_word(word='lue', distance=0.1520560692386022), Close_word(word='mew', distance=0.1520560692386022), Close_word(word='moo', distance=0.1520560692386022), Close_word(word='mu', distance=0.1520560692386022), Close_word(word='new', distance=0.1520560692386022), Close_word(word='nu', distance=0.1520560692386022), Close_word(word='pew', distance=0.1520560692386022), Close_word(word='phew', distance=0.1520560692386022), Close_word(word='phu', distance=0.1520560692386022), Close_word(word='plew', distance=0.1520560692386022), Close_word(word='pooh', distance=0.1520560692386022), Close_word(word='pu', distance=0.1520560692386022), Close_word(word='pugh', distance=0.1520560692386022), Close_word(word='queue', distance=0.1520560692386022), Close_word(word='rue', distance=0.1520560692386022), Close_word(word='schuh', distance=0.1520560692386022), Close_word(word='screw', distance=0.1520560692386022), Close_word(word='shoe', distance=0.1520560692386022), Close_word(word='shoo', distance=0.1520560692386022), Close_word(word='skew', distance=0.1520560692386022), Close_word(word='slew', distance=0.1520560692386022), Close_word(word='spew', distance=0.1520560692386022), Close_word(word='stew', distance=0.1520560692386022), Close_word(word='strew', distance=0.1520560692386022), Close_word(word='sue', distance=0.1520560692386022), Close_word(word='thew', distance=0.1520560692386022), Close_word(word='threw', distance=0.1520560692386022), Close_word(word='through', distance=0.1520560692386022), Close_word(word='true', distance=0.1520560692386022), Close_word(word='view', distance=0.1520560692386022), Close_word(word='whew', distance=0.1520560692386022), Close_word(word='who', distance=0.1520560692386022), Close_word(word='whoo', distance=0.1520560692386022), Close_word(word='woo', distance=0.1520560692386022), Close_word(word='yew', distance=0.1520560692386022), Close_word(word='you', distance=0.1520560692386022), Close_word(word='zoo', distance=0.1520560692386022)]), Phon_family(locus_word=Close_word(word='2', distance=0), close_words=[]), Phon_family(locus_word=Close_word(word='II', distance=0), close_words=[]), Phon_family(locus_word=Close_word(word='deuce', distance=0), close_words=[Close_word(word='boose', distance=0.13055929944741368), Close_word(word='cruce', distance=0.13055929944741368), Close_word(word='goose', distance=0.13055929944741368), Close_word(word='hoose', distance=0.13055929944741368), Close_word(word='juice', distance=0.13055929944741368), Close_word(word='loose', distance=0.13055929944741368), Close_word(word='luce', distance=0.13055929944741368), Close_word(word='moose', distance=0.13055929944741368), Close_word(word='mousse', distance=0.13055929944741368), Close_word(word='noose', distance=0.13055929944741368), Close_word(word='nous', distance=0.13055929944741368), Close_word(word='sluice', distance=0.13055929944741368), Close_word(word='spruce', distance=0.13055929944741368), Close_word(word='truce', distance=0.13055929944741368), Close_word(word='use', distance=0.13055929944741368)]), Phon_family(locus_word=Close_word(word='pair', distance=0.050000000000000044), close_words=[Close_word(word='aer', distance=0.1267643253108314), Close_word(word='air', distance=0.1267643253108314), Close_word(word='bare', distance=0.1267643253108314), Close_word(word='bear', distance=0.1267643253108314), Close_word(word='blair', distance=0.1267643253108314), Close_word(word='blare', distance=0.1267643253108314), Close_word(word='care', distance=0.1267643253108314), Close_word(word='chair', distance=0.1267643253108314), Close_word(word='dare', distance=0.1267643253108314), Close_word(word='darr', distance=0.1267643253108314), Close_word(word='ere', distance=0.1267643253108314), Close_word(word='err', distance=0.1267643253108314), Close_word(word='eyre', distance=0.1267643253108314), Close_word(word='fair', distance=0.1267643253108314), Close_word(word='fare', distance=0.1267643253108314), Close_word(word='flair', distance=0.1267643253108314), Close_word(word='flare', distance=0.1267643253108314), Close_word(word='gair', distance=0.1267643253108314), Close_word(word='gare', distance=0.1267643253108314), Close_word(word='glare', distance=0.1267643253108314), Close_word(word='hair', distance=0.1267643253108314), Close_word(word='haire', distance=0.1267643253108314), Close_word(word='hare', distance=0.1267643253108314), Close_word(word='heir', distance=0.1267643253108314), Close_word(word='lair', distance=0.1267643253108314), Close_word(word='lehr', distance=0.1267643253108314), Close_word(word='maire', distance=0.1267643253108314), Close_word(word='mare', distance=0.1267643253108314), Close_word(word='prayer', distance=0.1267643253108314), Close_word(word='rare', distance=0.1267643253108314), Close_word(word='sare', distance=0.1267643253108314), Close_word(word='scare', distance=0.1267643253108314), Close_word(word='share', distance=0.1267643253108314), Close_word(word='snare', distance=0.1267643253108314), Close_word(word='spare', distance=0.1267643253108314), Close_word(word='square', distance=0.1267643253108314), Close_word(word='stair', distance=0.1267643253108314), Close_word(word='stare', distance=0.1267643253108314), Close_word(word='swear', distance=0.1267643253108314), Close_word(word='tear', distance=0.1267643253108314), Close_word(word='their', distance=0.1267643253108314), Close_word(word='there', distance=0.1267643253108314), Close_word(word='ware', distance=0.1267643253108314), Close_word(word='wear', distance=0.1267643253108314), Close_word(word='where', distance=0.1267643253108314)]), Phon_family(locus_word=Close_word(word='twice', distance=0.09999999999999998), close_words=[Close_word(word='bice', distance=0.1332124921387603), Close_word(word='dice', distance=0.1332124921387603), Close_word(word='dyce', distance=0.1332124921387603), Close_word(word='feis', distance=0.1332124921387603), Close_word(word='gneiss', distance=0.1332124921387603), Close_word(word='grice', distance=0.1332124921387603), Close_word(word='ice', distance=0.1332124921387603), Close_word(word='mice', distance=0.1332124921387603), Close_word(word='nice', distance=0.1332124921387603), Close_word(word='price', distance=0.1332124921387603), Close_word(word='rice', distance=0.1332124921387603), Close_word(word='slice', distance=0.1332124921387603), Close_word(word='spice', distance=0.1332124921387603), Close_word(word='splice', distance=0.1332124921387603), Close_word(word='thrice', distance=0.1332124921387603), Close_word(word='tice', distance=0.1332124921387603), Close_word(word='trice', distance=0.1332124921387603), Close_word(word='vice', distance=0.1332124921387603), Close_word(word='vise', distance=0.1332124921387603), Close_word(word='wice', distance=0.1332124921387603)])])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sem_fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fancy-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_match_records = get_phrases_for_sem_fam( two_sem_fam )\n",
    "# word_match_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "mental-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be replaced with image recognition algorithms\n",
    "def get_image_topics():\n",
    "    return [test_image_topic]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-arkansas",
   "metadata": {},
   "source": [
    "## The equivalent of `main` for the time being, until two or more image topics are handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bacterial-secondary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close_word(word='two', distance=0)\n",
      "Close_word(word='2', distance=0)\n",
      "Close_word(word='II', distance=0)\n",
      "Close_word(word='deuce', distance=0)\n",
      "Close_word(word='pair', distance=0.050000000000000044)\n",
      "Close_word(word='twice', distance=0.09999999999999998)\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.050000000000000044\n",
      "0.09999999999999998\n"
     ]
    }
   ],
   "source": [
    "image_topics     = get_image_topics()\n",
    "image_topic_word = image_topics[0]\n",
    "\n",
    "two_sem_fam = make_phon_fams_and_sem_family( image_topic_word )\n",
    "#two_sem_fam\n",
    "\n",
    "word_match_records = get_phrases_for_sem_fam( two_sem_fam )\n",
    "\n",
    "cand_df = cand_df.append(word_match_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "figured-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cand_df = pd.DataFrame({\"semantic_match\": ['pair','pair','twice'], \"sem_dist\": [5, 5, 4.1], \"phonetic_match\": ['bear', 'hair','mice'], \"phon_dist\": [1.5, 2.7, 2.1], \"phrase_id\":  [ph_id1, ph_id2,ph_id3], \"phrase_type\":  ['idiom', 'idiom','idiom'],'score': [.5, .3, 1.1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "devoted-organizer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>semantic_match</th>\n",
       "      <th>sem_dist</th>\n",
       "      <th>phonetic_match</th>\n",
       "      <th>phon_dist</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>two</td>\n",
       "      <td>0.00</td>\n",
       "      <td>blue</td>\n",
       "      <td>0.197175</td>\n",
       "      <td>fcbcc188-807f-11eb-9b92-acde48001122</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>two</td>\n",
       "      <td>0.00</td>\n",
       "      <td>you</td>\n",
       "      <td>0.197175</td>\n",
       "      <td>fcbcb198-807f-11eb-9b92-acde48001122</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pair</td>\n",
       "      <td>0.05</td>\n",
       "      <td>air</td>\n",
       "      <td>0.108329</td>\n",
       "      <td>fcbcb6e8-807f-11eb-9b92-acde48001122</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pair</td>\n",
       "      <td>0.05</td>\n",
       "      <td>bear</td>\n",
       "      <td>0.108329</td>\n",
       "      <td>fcbc9eec-807f-11eb-9b92-acde48001122</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pair</td>\n",
       "      <td>0.05</td>\n",
       "      <td>hair</td>\n",
       "      <td>0.108329</td>\n",
       "      <td>fcbca676-807f-11eb-9b92-acde48001122</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>twice</td>\n",
       "      <td>0.10</td>\n",
       "      <td>mice</td>\n",
       "      <td>0.188696</td>\n",
       "      <td>fcbcac2a-807f-11eb-9b92-acde48001122</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>twice</td>\n",
       "      <td>0.10</td>\n",
       "      <td>nice</td>\n",
       "      <td>0.188696</td>\n",
       "      <td>fcbcbc1a-807f-11eb-9b92-acde48001122</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  semantic_match  sem_dist phonetic_match  phon_dist  \\\n",
       "0            two      0.00           blue   0.197175   \n",
       "1            two      0.00            you   0.197175   \n",
       "2           pair      0.05            air   0.108329   \n",
       "3           pair      0.05           bear   0.108329   \n",
       "4           pair      0.05           hair   0.108329   \n",
       "5          twice      0.10           mice   0.188696   \n",
       "6          twice      0.10           nice   0.188696   \n",
       "\n",
       "                              phrase_id score  \n",
       "0  fcbcc188-807f-11eb-9b92-acde48001122        \n",
       "1  fcbcb198-807f-11eb-9b92-acde48001122        \n",
       "2  fcbcb6e8-807f-11eb-9b92-acde48001122        \n",
       "3  fcbc9eec-807f-11eb-9b92-acde48001122        \n",
       "4  fcbca676-807f-11eb-9b92-acde48001122        \n",
       "5  fcbcac2a-807f-11eb-9b92-acde48001122        \n",
       "6  fcbcbc1a-807f-11eb-9b92-acde48001122        "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adjusted-berlin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>semantic_match</th>\n",
       "      <th>sem_dist</th>\n",
       "      <th>phonetic_match</th>\n",
       "      <th>phon_dist</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>two</td>\n",
       "      <td>0.00</td>\n",
       "      <td>blue</td>\n",
       "      <td>0.197175</td>\n",
       "      <td>fcbcc188-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>5.071638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>two</td>\n",
       "      <td>0.00</td>\n",
       "      <td>you</td>\n",
       "      <td>0.197175</td>\n",
       "      <td>fcbcb198-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>5.071638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pair</td>\n",
       "      <td>0.05</td>\n",
       "      <td>air</td>\n",
       "      <td>0.108329</td>\n",
       "      <td>fcbcb6e8-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>6.315974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pair</td>\n",
       "      <td>0.05</td>\n",
       "      <td>bear</td>\n",
       "      <td>0.108329</td>\n",
       "      <td>fcbc9eec-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>6.315974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pair</td>\n",
       "      <td>0.05</td>\n",
       "      <td>hair</td>\n",
       "      <td>0.108329</td>\n",
       "      <td>fcbca676-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>6.315974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>twice</td>\n",
       "      <td>0.10</td>\n",
       "      <td>mice</td>\n",
       "      <td>0.188696</td>\n",
       "      <td>fcbcac2a-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>3.463856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>twice</td>\n",
       "      <td>0.10</td>\n",
       "      <td>nice</td>\n",
       "      <td>0.188696</td>\n",
       "      <td>fcbcbc1a-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>3.463856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  semantic_match  sem_dist phonetic_match  phon_dist  \\\n",
       "0            two      0.00           blue   0.197175   \n",
       "1            two      0.00            you   0.197175   \n",
       "2           pair      0.05            air   0.108329   \n",
       "3           pair      0.05           bear   0.108329   \n",
       "4           pair      0.05           hair   0.108329   \n",
       "5          twice      0.10           mice   0.188696   \n",
       "6          twice      0.10           nice   0.188696   \n",
       "\n",
       "                              phrase_id     score  \n",
       "0  fcbcc188-807f-11eb-9b92-acde48001122  5.071638  \n",
       "1  fcbcb198-807f-11eb-9b92-acde48001122  5.071638  \n",
       "2  fcbcb6e8-807f-11eb-9b92-acde48001122  6.315974  \n",
       "3  fcbc9eec-807f-11eb-9b92-acde48001122  6.315974  \n",
       "4  fcbca676-807f-11eb-9b92-acde48001122  6.315974  \n",
       "5  fcbcac2a-807f-11eb-9b92-acde48001122  3.463856  \n",
       "6  fcbcbc1a-807f-11eb-9b92-acde48001122  3.463856  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cand_df['score'] = cand_df.apply(lambda row: 1.0/(row['sem_dist'] + row['phon_dist']), axis=1)\n",
    "cand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-lightning",
   "metadata": {},
   "source": [
    "## 5 —  Choose captions\n",
    "\n",
    "Sort captions dataframe by the _suitability score_\n",
    "\n",
    "Choose the top 10(?) and generate a list containing each caption with the original semantic family word substituted into the idiom in addition to jokes containing any of the semantic family words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bored-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub(phrase, orig_word='', new_word=''):\n",
    "    return phrase.text_string.replace(orig_word, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-python",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "nominated-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_caption( row ):\n",
    "    return sub( phrase_dict[ row['phrase_id']], row['phonetic_match'],  row['semantic_match']  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "studied-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_captions(df, selection_size=10):\n",
    "    df.sort_values(by='score', inplace=True)\n",
    "    best_df = df.head(selection_size)\n",
    "    #best_df['caption'] = best_df.apply(construct_caption, axis=1 )\n",
    "    best_df.loc[:, 'caption'] = best_df.apply(construct_caption, axis=1 ) \n",
    "    return best_df\n",
    "    #return best_df['_caption'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "german-appearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/metis/lib/python3.8/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/opt/anaconda3/envs/metis/lib/python3.8/site-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>semantic_match</th>\n",
       "      <th>sem_dist</th>\n",
       "      <th>phonetic_match</th>\n",
       "      <th>phon_dist</th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>score</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>twice</td>\n",
       "      <td>0.10</td>\n",
       "      <td>mice</td>\n",
       "      <td>0.188696</td>\n",
       "      <td>fcbcac2a-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>3.463856</td>\n",
       "      <td>Three blind twice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>twice</td>\n",
       "      <td>0.10</td>\n",
       "      <td>nice</td>\n",
       "      <td>0.188696</td>\n",
       "      <td>fcbcbc1a-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>3.463856</td>\n",
       "      <td>Wouldn't it be twice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>two</td>\n",
       "      <td>0.00</td>\n",
       "      <td>blue</td>\n",
       "      <td>0.197175</td>\n",
       "      <td>fcbcc188-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>5.071638</td>\n",
       "      <td>Roses are red, violets are two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>two</td>\n",
       "      <td>0.00</td>\n",
       "      <td>you</td>\n",
       "      <td>0.197175</td>\n",
       "      <td>fcbcb198-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>5.071638</td>\n",
       "      <td>I just called to say I love two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pair</td>\n",
       "      <td>0.05</td>\n",
       "      <td>air</td>\n",
       "      <td>0.108329</td>\n",
       "      <td>fcbcb6e8-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>6.315974</td>\n",
       "      <td>Up, up in the pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pair</td>\n",
       "      <td>0.05</td>\n",
       "      <td>bear</td>\n",
       "      <td>0.108329</td>\n",
       "      <td>fcbc9eec-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>6.315974</td>\n",
       "      <td>Smarter than the average pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pair</td>\n",
       "      <td>0.05</td>\n",
       "      <td>hair</td>\n",
       "      <td>0.108329</td>\n",
       "      <td>fcbca676-807f-11eb-9b92-acde48001122</td>\n",
       "      <td>6.315974</td>\n",
       "      <td>Not a pair out of place</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  semantic_match  sem_dist phonetic_match  phon_dist  \\\n",
       "5          twice      0.10           mice   0.188696   \n",
       "6          twice      0.10           nice   0.188696   \n",
       "0            two      0.00           blue   0.197175   \n",
       "1            two      0.00            you   0.197175   \n",
       "2           pair      0.05            air   0.108329   \n",
       "3           pair      0.05           bear   0.108329   \n",
       "4           pair      0.05           hair   0.108329   \n",
       "\n",
       "                              phrase_id     score  \\\n",
       "5  fcbcac2a-807f-11eb-9b92-acde48001122  3.463856   \n",
       "6  fcbcbc1a-807f-11eb-9b92-acde48001122  3.463856   \n",
       "0  fcbcc188-807f-11eb-9b92-acde48001122  5.071638   \n",
       "1  fcbcb198-807f-11eb-9b92-acde48001122  5.071638   \n",
       "2  fcbcb6e8-807f-11eb-9b92-acde48001122  6.315974   \n",
       "3  fcbc9eec-807f-11eb-9b92-acde48001122  6.315974   \n",
       "4  fcbca676-807f-11eb-9b92-acde48001122  6.315974   \n",
       "\n",
       "                           caption  \n",
       "5                Three blind twice  \n",
       "6             Wouldn't it be twice  \n",
       "0   Roses are red, violets are two  \n",
       "1  I just called to say I love two  \n",
       "2               Up, up in the pair  \n",
       "3    Smarter than the average pair  \n",
       "4          Not a pair out of place  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_captions_df = get_best_captions(cand_df)\n",
    "best_captions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cosmetic-tower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Three blind twice',\n",
       " \"Wouldn't it be twice\",\n",
       " 'Roses are red, violets are two',\n",
       " 'I just called to say I love two',\n",
       " 'Up, up in the pair',\n",
       " 'Smarter than the average pair',\n",
       " 'Not a pair out of place']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_captions_list = best_captions_df['caption'].to_list()\n",
    "best_captions_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-tuner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-bracelet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-nickel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-charity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-cowboy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
